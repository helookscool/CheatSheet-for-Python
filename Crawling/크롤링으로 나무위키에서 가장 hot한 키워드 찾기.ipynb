{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91da1dcf",
   "metadata": {},
   "source": [
    "### 나무위키 '최근 변경 페이지(https://namu.wiki/RecentChanges)'의 텍스트 데이터를 웹 크롤링으로 수집한 다음, 데이터 내에서 등장한 키워드의 출현빈도를 분석해 보겠습니다. \n",
    "\n",
    "-가장 먼저 해야할 일은 페이지 리스트의 URL정보를 수집하는 것입니다. \n",
    "-파이선에서는 BeautifulSoup와 requests라는 라이브러리로 웹 크롤러를 만들 수 있습니다. requests는 특정 URL로부터 HTML문서를 가져오는 작업을 수행하고, BeautifulSoup모듈은 HTML문서에서 데이터를 추출하는 작업을 수행합니다. \n",
    "\n",
    "-아래 코드에서는 requests.get()함수로 URL의 HTML문서를 가져온 뒤, 이를 BeautifulSoup()클래스의 soup객체로 변환합니다. 그리고 find(), find_all()함수를 사용하여 특정 HTML태그 혹은 특정 HTML 클래스를 가진 데이터를 가져옵니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec123be4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T03:18:31.927007Z",
     "start_time": "2021-07-27T03:18:29.325088Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# 윈도우용 크롬 웹드라이버 실행 경로 (Windows)\n",
    "excutable_path = \"C:/Users/MSI/Desktop/chromedriver_win32/chromedriver.exe\"\n",
    "\n",
    "# 크롤링할 사이트 주소를 정의합니다.\n",
    "source_url = \"https://namu.wiki/RecentChanges\"\n",
    "\n",
    "# 사이트의 html 구조에 기반하여 크롤링을 수행합니다.\n",
    "# driver = webdriver.Chrome(path)  # for Mac\n",
    "driver = webdriver.Chrome(executable_path=excutable_path)  # for Windows\n",
    "driver.get(source_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f02fc346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T03:18:36.268731Z",
     "start_time": "2021-07-27T03:18:36.093732Z"
    }
   },
   "outputs": [],
   "source": [
    "#아래 명령어들을 실행하려면 위의 명령어를 통해 소환된 웹페이지를 닫지 않아야함.\n",
    "req = driver.page_source\n",
    "soup = BeautifulSoup(req, \"html.parser\")\n",
    "#div =  soup.find('div', id='app')\n",
    "contents_table = soup.find(name=\"table\")\n",
    "table_body = contents_table.find(name=\"tbody\")\n",
    "table_rows = table_body.find_all(name=\"tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c851bdfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T03:18:43.715395Z",
     "start_time": "2021-07-27T03:18:43.621233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://namu.wiki/w/%EC%99%80%EC%81%A0\n",
      "https://namu.wiki/w/Friday%20Night%20Funkin'/%EC%8A%A4%ED%85%8C%EC%9D%B4%EC%A7%80\n",
      "https://namu.wiki/w/%EB%82%98%EC%97%90%EA%B2%8C%20%EC%B2%9C%EC%82%AC%EA%B0%80%20%EB%82%B4%EB%A0%A4%EC%99%94%EB%8B%A4!\n",
      "https://namu.wiki/w/2016%EB%85%84\n",
      "https://namu.wiki/w/%EC%9A%B4%ED%95%98%EC%B6%A9\n"
     ]
    }
   ],
   "source": [
    "# a태그의 href 속성을 리스트로 추출하여, 크롤링 할 페이지 리스트를 생성합니다.\n",
    "page_url_base = \"https://namu.wiki\"\n",
    "page_urls = []\n",
    "for index in range(0, len(table_rows)):\n",
    "    first_td = table_rows[index].find_all(\"td\")[0]\n",
    "    td_url = first_td.find_all(\"a\")\n",
    "    if len(td_url) > 0:\n",
    "        page_url = page_url_base + td_url[0].get(\"href\")\n",
    "        if \"png\" not in page_url:\n",
    "            page_urls.append(page_url)\n",
    "\n",
    "# 중복 url을 제거합니다.\n",
    "page_urls = list(set(page_urls))\n",
    "for page in page_urls[:5]:\n",
    "    print(page)\n",
    "\n",
    "# 크롤링에 사용한 브라우저를 종료합니다.\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0a33bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T03:24:09.426898Z",
     "start_time": "2021-07-27T03:24:05.214545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "와쁠 \n",
      "\n",
      "\n",
      "유튜버/ㅇ쿠키런 시리즈/게이머2001년 출생\n",
      "\n",
      "\n",
      "패러블 엔터테인먼트 소속 스트리머 [ 펼치기 · 접기 ]엔터테이너김수현조매력크리에이터369랑께개구몽개복어갱복치고먐잉꽈뚜룹노돌리다피달디달러왕두칠로빈마루에몽맥또마또메탈킴무결미요박나나블루샤크서커스진세치혀신속쌔오영래기오벨오요와쁠우왁굳주르르진호찌모햄천양텔론툭툭포베함군해봄허샘헤가히똥이엔터테이너김수현조매력크리에이터369랑께개구몽개복어갱복치고먐잉꽈뚜룹노돌리다피달디달러왕두칠로빈마루에몽맥또마또메탈킴무결미요박나나블루샤크서커스진세치혀신속쌔오영래기오벨오요와쁠우왁굳주르르진호찌모햄천양텔론툭툭포베함군해봄허샘헤가히똥이와쁠본명조정윤출생2001년 3월 22일국적대한민국 직업유튜버개설일2015년 5월 5일 +2275일 째구독자8만명[기준]조회수45,196,187회[기준]링크  1. 개요2. 상세3. 쿠키런: 오븐브레이크3.1. 떼탈출3.2. 랜드3.3. 챔피언스리그3.4. 길드4. 여담모바일 게임 유튜버 와쁠 입니다.소개모바일 게임 유튜버.유튜브 스트리밍과[2] 트위치를 통해 방송한다. 캠은 없지만 목소리가 굉장히 좋은 편. ASMR 할 생각 없냐고 물어보면...시간대는 랜덤[4]이나 주로 아침이나 점심에 진행한다. 본인은 1시간 방송이라 선언하지만 정작 1시간 딱 방송하고 종료하는 경우는 거의 없다.주로 쿠키런: 오븐브레이크를 플레이하지만 본인의 기분에 따라 LOL이나 TFT를 플레이하기도 한다. 2021년 초에 쿠키런: 킹덤이 출시하면서 쿠킹덤도 주 플레이 게임이 되었다.게임을 하면서 계속 말을 한다. 쿠키런에서 막판에 점프할 때마다 '발악, 점프!'라고 외치는 특징이 있다. 길드전을 하다가 장애물에 부딪히는 등의 실수를 하면 유리창 깨지는 효과음이 나고, 와쁠은 그때마다 뻔뻔하게 사소한 거라고 하는 게 포인트.[5] 그 밖에도 종종 우스갯소리로 자기합리화를 하는데 팬들은 이를 '쁠소리'라고 부른다.[6]물론 이후 다시 제대로 플레이하는 편.떼탈출을 자주 달리는 편이 아니다대신 랭킹 1위와 함께 진행하는 떼탈출 장인초대석 컨텐츠를 진행한다쿠키런의 변태적인 빌드를 랭킹 1위의 즉석 훈수[7]로 습득해나가는 모습을 보여준다떼탈출 에피소드1에서 레전드, 스페셜 쿠키 없이 에픽 쿠키와 용사맛 쿠키만으로 30억을 찍었다.마녀의 오븐탈출에서 와사비의 추천조합[8]으로 상위 7.7%를 찍었다.와쁠 유튜브의 메인 컨텐츠. 랭커 빌드는 물론, 뉴메타나 신쿠키 위주의 빌드를 정리하여 영상을 만든다.생방송에서 빌드를 직접 연습하는 모습도 보이는데, 어려운 조합의 경우 1시간 이상 재도전하며 멘탈이 나간 모습을 보일 때도 있다.마스터랜드가 출시되면서 조합 랭킹표 상위권 빌드의 점수를 갈아치우는(...) 역할을 하고 있다.평소에는 하지 않고, 마지막 챔피언스리그에 골드 티켓[9]을 얻기 위해 몰아서(...) 한다.그랜드 챔피언스리그 시기에는 컨텐츠를 위해 열심히 하는 모습을 보이기도 한다. 그랜드 챔피언스리그 8강 진출 경험이 있을 정도로 실력은 있는 편. 다만 컨텐츠를 하려다 일정 잘못 맞춰서 그대로 참가 못한 적도 있다. 0라딱(...)와쁠youtube 길드의 길드 마스터이다.길드원의 재촉으로 어쩔 수 없이 길드전에 참가할 때도 있지만, 보통은 길드원에게 티켓을 보내기만 하고 길드전에 참여하지 않는다. 길드원이 빡쳐서 쓴 내용인가?2020년 10월 기준, 위의 내용과는 다르게 컨텐츠를 뽑기 위해 생방에서 길드전을 많이 돌리는 모습을 보인다. 유튜브에도 길드전 빌드가 자주 올라오는 추세이다.2013년도부터 쿠키런을 플레이했던 쿠키런 골수 유저이다.와쁠이란 닉네임은 아무 생각없이 지은 닉네임이다.키가 185cm로 상당히 큰 편이다학창 시절에 공부, 특히 수학을 잘했다고 한다.[10]인터넷 상에서 도를 넘고 예의가 없는 초등학생을 매우 싫어한다.[11] 때문에 생방송 시 방송규칙이 굉장히 빡빡하기로 악명 높은데, 압권은 15세 미만 시청금지(...).쿠키 중에선 샤벳상어맛 쿠키를 가장 좋아한다. 이유는 샤벳상어의 보탐 모션.술 마시는 것을 좋아한다고 한다. 집에 맥주를 항상 구비해놓는다고.최저시급을 받으면 방송시간이 늘어나는 등 돈미새 기질도 보인다.논란이 생기는 멘트를 굉장히 꺼리는 편이다.생방송 중에 이클립스를 자주 먹는다.망고맛 쿠키의 스킨을 얻기 위해 15만원을 현질[12]하고, 바다요정 쿠키의 레전더리 스킨을 얻기 위해 18만원을 현질하는 등 스킨뽑기에 진심인 편이다매 시즌 플레티넘을 달성하는 등 LOL[13]과 TFT의 상당한 실력을 보유하고 있다.BMI 지수 미달로 공익 판정을 받았다.MBTI는 INTJ로 판정되었다.2020년 5월 20일 ND러너, 캐빠기YC와 합방 중 귀여운 스타일이 이상형이라고 밝혔다.2020년 5월 21일 기준 카트라이더 러쉬플러스를 즐겨 한다고 한다.2020년 10월 방송에서 동갑의 롤 뉴메타 유튜버 부멘과 친해지고, 롤 듀오하는 모습을 보였다. 세기말 다이아 등반 중.민트초코를 싫어한다.LOL주챔은 티모이다.확률과 통계를 좋아한다. 쿠키런: 킹덤에서 슈크림맛 쿠키와 악연이 있다. 엄청난 과금을 해도 오랜기간 뽑기에서 안나온 상태라 '현질해도 무과금' 소리까지 나올 정도. 하도 안나와서 유튜브 커뮤니티에서 하소연을 하기도 했다. 5월달 마침내 슈크림맛 쿠키 뽑기에 성공했다.여담으로 손이 굉장히 예쁘다.[기준] 1.1 1.2 2021년 6월 19일.[2] 유튜브 스트리밍은 접었다.[4] 본인의 수면 패턴이 랜덤이라 그렇다.[5] 다른 쿠키런 유투버들도 이 사소하다는 말을 가끔씩 따라하기도 할 정도.[6] 어감 상 허튼소리의 방언인 '뻘소리'에서 유래한 듯.[7] 디스코드 화면공유를 통해 빌드나 주의해야할 점들을 즉각적으로 알려준다[8] 이라고 쓰고 최신순 조합 이라고 읽는다...[9] 그랜드 챔피언스리그에 참가할 수 있는 티켓.[10] 커뮤니티 글에 자신의 방 사진이 올라와있는데 KMO 문제집이 있는것을 보면 공부를 잘했다는게 틀린말은 아닌듯[11] 아이러니하게도 쿠키런 컨텐츠의 주 소비층은 잼민이로 대표되는 저연령층이다.[12] 참고로 해당 스킨은 레어 등급이라 그냥 무지개 큐브를 주고 살 수도 있었다(...).[13] 주 라인은 정글.\n"
     ]
    }
   ],
   "source": [
    "# driver = webdriver.Chrome(path)  # for Mac\n",
    "driver = webdriver.Chrome(executable_path=excutable_path)  # for Windows\n",
    "driver.get(page_urls[0])\n",
    "req = driver.page_source\n",
    "soup = BeautifulSoup(req, 'html.parser')\n",
    "contents_table = soup.find(name=\"article\")\n",
    "title = contents_table.find_all('h1')[0]\n",
    "category = contents_table.find_all('ul')[0]\n",
    "content_paragraphs = contents_table.find_all(\n",
    "    name=\"div\", attrs={\"class\":\"wiki-paragraph\"})\n",
    "content_corpus_list = []\n",
    "\n",
    "for paragraphs in content_paragraphs:\n",
    "    content_corpus_list.append(paragraphs.text)\n",
    "content_corpus = \"\".join(content_corpus_list)\n",
    "\n",
    "print(title.text)\n",
    "print(\"\\n\")\n",
    "print(category.text)\n",
    "print(\"\\n\")\n",
    "print(content_corpus)\n",
    "\n",
    "# 크롤링에 사용한 브라우저를 종료합니다.\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4382c8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T06:08:37.533892Z",
     "start_time": "2021-07-27T06:08:33.539Z"
    }
   },
   "outputs": [],
   "source": [
    "# 크롤링한 데이터를 데이터 프레임으로 만들기 위해 준비합니다.\n",
    "columns = [\"title\", \"category\", \"content_text\"]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# 각 페이지별 '제목', '카테고리', '본문' 정보를 데이터 프레임으로 만듭니다.\n",
    "for page_url in page_urls:\n",
    "\n",
    "    # 사이트의 html 구조에 기반하여 크롤링을 수행합니다.\n",
    "    # driver = webdriver.Chrome(path)  # for Mac\n",
    "    driver = webdriver.Chrome(executable_path=excutable_path)  # for Windows\n",
    "    driver.get(page_url)\n",
    "    req = driver.page_source\n",
    "    soup = BeautifulSoup(req, \"html.parser\")\n",
    "    contents_table = soup.find(name=\"article\")\n",
    "    title = contents_table.find_all(\"h1\")[0]\n",
    "    \n",
    "    # 카테고리 정보가 없는 경우를 확인합니다.\n",
    "    if len(contents_table.find_all(\"ul\")) > 0:\n",
    "        category = contents_table.find_all(\"ul\")[0]\n",
    "    else:\n",
    "        category = None\n",
    "        \n",
    "    content_paragraphs = contents_table.find_all(\n",
    "        name=\"div\", attrs={\"class\":\"wiki-paragraph\"})\n",
    "    content_corpus_list = []\n",
    "    \n",
    "    # 페이지 내 제목 정보에서 개행 문자를 제거한 뒤 추출합니다. 만약 없는 경우, 빈 문자열로 대체합니다.\n",
    "    if title is not None:\n",
    "        row_title = title.text.replace(\"\\n\", \" \")\n",
    "    else:\n",
    "        row_title = \"\"\n",
    "    \n",
    "    # 페이지 내 본문 정보에서 개행 문자를 제거한 뒤 추출합니다. 만약 없는 경우, 빈 문자열로 대체합니다.\n",
    "    if content_paragraphs is not None:\n",
    "        for paragraphs in content_paragraphs:\n",
    "            if paragraphs is not None:\n",
    "                content_corpus_list.append(paragraphs.text.replace(\"\\n\", \" \"))\n",
    "            else:\n",
    "                content_corpus_list.append(\"\")\n",
    "    else:\n",
    "        content_corpus_list.append(\"\")\n",
    "        \n",
    "    # 페이지 내 카테고리정보에서 “분류”라는 단어와 개행 문자를 제거한 뒤 추출합니다. 만약 없는 경우, 빈 문자열로 대체합니다.\n",
    "    if category is not None:\n",
    "        row_category = category.text.replace(\"\\n\", \" \")\n",
    "    else:\n",
    "        row_category = \"\"\n",
    "    \n",
    "    # 모든 정보를 하나의 데이터 프레임에 저장합니다.\n",
    "    row = [row_title, row_category, \"\".join(content_corpus_list)]\n",
    "    series = pd.Series(row, index=df.columns)\n",
    "    df = df.append(series, ignore_index=True)\n",
    "    \n",
    "    # 크롤링에 사용한 브라우저를 종료합니다.\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4362919",
   "metadata": {},
   "outputs": [],
   "source": [
    "이어서 계속...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
